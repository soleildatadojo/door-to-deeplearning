{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 「**Deep Learning**の扉」　　\n",
    "### ④**Python**を使ってニューラルネットワークの**誤差逆伝播法**を学ぶPart2\n",
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.誤差逆伝播についてより深く！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノートブック「DeepLearning4_1」では、基礎的な話をメインに説明してきました。\n",
    "\n",
    "計算グラフや局所的な微分、連鎖律など誤差逆伝播に必要な知識となっています。\n",
    "\n",
    "最終的には、リンゴやみかんの例を用いて、加算レイヤや乗算レイヤを実装しました。\n",
    "\n",
    "計算する際、たくさんの記述を必要としましたが、内容は簡単であり、理解しやすかったと思います。\n",
    "\n",
    "このノートブックでは、基礎知識をニューラルネットワークに活かしていきたいと思います！！\n",
    "\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、計算グラフをニューラルネットワークに適用したいと思います。\n",
    "\n",
    "ここでは、ニューラルネットワークを構成する「層（レイヤ）」を一つのクラスとして実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1.ReLUレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数である「ReLU」を覚えていますでしょうか？？\n",
    "\n",
    "「ReLU」は以下のような式で書くことができました。\n",
    "\n",
    "$$\n",
    "    y = \\left\\{\n",
    "    \\begin{array}{}\n",
    "    x & (x > 0)\\\\\n",
    "    0 & (x \\leq 0)\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グラフにすると、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-1, 1, 0.1)\n",
    "y = ReLU(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように表すことができます。\n",
    "\n",
    "また、xに関するyの微分は次式のように求められます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\frac{∂y}{∂x} = \\left\\{\n",
    "    \\begin{array}{}\n",
    "    1 & (x > 0)\\\\\n",
    "    0 & (x \\leq 0)\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらのことから、順伝播時の入力であるxが0より大きければ、逆伝播は上流の値をそのまま下流に流します。\n",
    "\n",
    "逆に、順伝播時にxが０以下であれば、逆伝播では下流への信号はそこでストップします。\n",
    "\n",
    "計算グラフで表すと以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_1.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、このReLUレイヤの実装を行いましょう。\n",
    "\n",
    "ニューラルネットワークのレイヤの実装では、Numpyの配列が入力されることを想定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reluクラスは、インスタンス変数としてmaskという変数を持ちます。\n",
    "\n",
    "このmask変数は「True/False」からなるNumpy配列で、\n",
    "\n",
    "#### ・順伝播の入力であるxの要素で０以下：True\n",
    "\n",
    "#### ・それ以外（０より大きい）：False\n",
    "\n",
    "となります。\n",
    "\n",
    "では、具体例を用いて見ていきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [False False]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.1], [0.2, 0.5]])\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、数値が格納されていた配列xに対し、配列maskは「True/False」が格納されていることがわかります。\n",
    "\n",
    "また、配列xにおいて、maskを指定することで、Trueの値を得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを利用することによって、ReLUの特性を再現することができます。\n",
    "\n",
    "先ほどの例を使うと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. ],\n",
       "       [0.2, 0.5]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = x.copy()\n",
    "out[mask] = 0\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、「True」のところに「0」を代入することができています。\n",
    "\n",
    "また、「self.mask」を使って「True/False」を保持することで、\n",
    "\n",
    "逆伝播時に、上流から伝播された「dout」に対して、「mask」の要素が「True」の場所を0にすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2-1】ReLUレイヤを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######以下にコードを書いてください######\n",
    "class Relu:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######「Shift+Enter」を押してください######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2.Sigmoidレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、シグモイド関数を実装していきます。\n",
    "\n",
    "シグモイド関数は以下のような式で表されます。\n",
    "\n",
    "$$\n",
    "    y = \\frac{1}{1+exp(-x)}\n",
    "$$\n",
    "\n",
    "また、上式を計算グラフで表すと以下のような図になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_2.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上図では、「exp」と「/」ノードが新しく登場しています。\n",
    "\n",
    "「exp」ノードでは、\n",
    "\n",
    "$$\n",
    "    y = exp(x)\n",
    "$$\n",
    "\n",
    "の計算を行い、\n",
    "\n",
    "「/」ノードは、\n",
    "\n",
    "$$\n",
    "    y = \\frac{1}{x}\n",
    "$$\n",
    "\n",
    "の計算を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、Sigmoidの逆伝播を見ていきたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### ・step1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「/」ノードは\n",
    "\n",
    "$$\n",
    "    y = \\frac{1}{x}\n",
    "$$\n",
    "\n",
    "を表しますが、この微分は次式のように表すことができます。\n",
    "\n",
    "$$\n",
    "    \\frac{∂y}{∂x} = -\\frac{1}{x^2}\n",
    "$$\n",
    "\n",
    "また、\n",
    "\n",
    "$$\n",
    "    y^2 = \\frac{1}{x^2}\n",
    "$$\n",
    "\n",
    "なので、微分の式に代入すると、\n",
    "\n",
    "$$\n",
    "    \\frac{∂y}{∂x} = -y^2\n",
    "$$\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このことから、逆伝播の時は、上流の値に対して、「$-y^2$」を乗算して下流へ伝播することがわかります。\n",
    "\n",
    "ここでいう「y」は順伝播時の出力になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ・step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「+」ノードは前回のノートブックで説明したように、上流の値をそのまま下流に流すだけになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ・step3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「exp」ノードは\n",
    "\n",
    "$$\n",
    "    y = exp(x)\n",
    "$$\n",
    "\n",
    "を表し、その微分は次の式で表されます。\n",
    "\n",
    "$$\n",
    "    \\frac{∂y}{∂x} = exp(x)\n",
    "$$\n",
    "\n",
    "指数関数の微分を求める場合、$exp(x)$という形はそのままで、\n",
    "\n",
    "中身の「x」の微分を考えます。\n",
    "\n",
    "そして、「x」の微分の結果と「exp(x)」の積を考えます。\n",
    "\n",
    "つまり、\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    \\frac{∂y}{∂x} &= \\frac{∂x}{∂x}\\times exp(x)\\\\\n",
    "    &= 1\\times exp(x)\\\\\n",
    "    &= exp(x)\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらのことから逆伝播の際、「exp(-x)」 を上流の値に乗算して下流へと伝播します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ・step4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「×」ノードは、順伝播時の値をひっくり返して乗算すると、前回のノートブックで説明しました。\n",
    "\n",
    "そのため、ここでは「-1」を乗算します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの結果を計算グラフに表すと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_3.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上より、Sigmoidレイヤの逆伝播を行うことができました。\n",
    "\n",
    "計算グラフの結果より、逆伝播の出力は\n",
    "\n",
    "$$\n",
    "    \\frac{∂L}{∂y}y^{2}exp(-x)\n",
    "$$\n",
    "\n",
    "となり、この値が下流にあるノードに伝播していきます。\n",
    "\n",
    "ここで、\n",
    "\n",
    "$$\n",
    "    \\frac{∂L}{∂y}y^{2}exp(-x)\n",
    "$$\n",
    "\n",
    "は、順伝播の入力xと出力yだけから計算できる点に注目しましょう。\n",
    "\n",
    "言い換えると、Sigmoidの逆伝播の出力には、順伝播の入力xと出力yを保持する必要があるということになります。\n",
    "\n",
    "これらのことを踏まえて、「sigmoid」ノードを図で表すと、以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_4.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中身の計算を詳しく書いたものと、上図のように簡略的に書いたものでは、計算の結果は同じものになります。\n",
    "\n",
    "しかし、簡略的に書いた計算グラフの方が、効率の良い計算と言えます。\n",
    "\n",
    "また、ノードをグループ化することによって、Sigmoidレイヤの細かい中身を気にすることなく、その入力と出力だけに集中することができる点も重要なポイントとなっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで\n",
    "\n",
    "$$\n",
    "    \\frac{∂L}{∂y}y^{2}exp(-x)\n",
    "$$\n",
    "\n",
    "に工夫を加えたいと思います。\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    \\frac{∂L}{∂y}y^{2}exp(-x) &= \\frac{∂L}{∂y}\\frac{1}{(1+exp(-x))^2}exp(-x)\\\\\n",
    "    &= \\frac{∂L}{∂y}\\frac{1}{1+exp(-x)}\\frac{exp(-x)}{1+exp(-x)}\\\\\n",
    "    &= \\frac{∂L}{∂y}y\\frac{1+exp(-x)-1}{1+exp(-x)}\\\\\n",
    "    &= \\frac{∂L}{∂y}y(\\frac{1+exp(-x)}{1+exp(-x)}-\\frac{1}{1+exp(-x)})\\\\\n",
    "    &= \\frac{∂L}{∂y}y(1-y)\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この式から、Sigmoidレイヤの逆伝播は、順伝播の出力だけから計算することがわかります。\n",
    "\n",
    "これを踏まえて、計算グラフを書き直すと以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_5.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、SigmoidレイヤをPythonで実装していきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この実装では、順伝播時に出力をインスタンス変数のoutに保持しておき、逆伝播時に、この値を利用します。\n",
    "\n",
    "このように計算グラフを使って、一つ一つ確認することによって、簡単にレイヤを実装することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2-2】Sigmoidレイヤを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######以下にコードを書いてください######\n",
    "\n",
    "\n",
    "\n",
    "######「Shift+Enter」を押してください######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Affineレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの順伝播では、重み付き信号の総和を計算するために、行列の積を用いました。\n",
    "\n",
    "例えば、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力Xの形状： (2,)\n",
      "重みWの形状： (2, 3)\n",
      "バイアスBの形状 (3,)\n",
      "出力値： [1.22 1.43 1.64]\n",
      "出力Yの形状： (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X= np.array([0.1, 0.2])\n",
    "W = np.array([[0.2, 0.3, 0.4], [1, 2, 3]])\n",
    "B = np.array([1, 1, 1])\n",
    "\n",
    "print(\"入力Xの形状：\", X.shape)\n",
    "print(\"重みWの形状：\", W.shape)\n",
    "print(\"バイアスBの形状\", B.shape)\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "\n",
    "print(\"出力値：\", Y)\n",
    "print(\"出力Yの形状：\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "といった計算を行いました。\n",
    "\n",
    "このように行列Aと行列Bの積を計算する時は、行列Aの列数と行列Bの行数を揃える必要があります。\n",
    "\n",
    "また、「Y = np.dot(X, W) + B」と計算した後、Yは活性化関数によって変換され、次の層に伝播される、というのがニューラルネットワークの順伝播の流れになりました。\n",
    "\n",
    "それでは、ここで行った計算を計算グラフで表してみましょう。\n",
    "\n",
    "行列の積を計算するノードを「dot」として表すことにすると、\n",
    "\n",
    "「np.dot(X, W) + B」の計算は、以下の計算グラフで表すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_6.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上図は比較的簡単な計算グラフになります。\n",
    "\n",
    "ただし、XやW、Bというのは、行列であるということに注意してください。\n",
    "\n",
    "それでは逆伝播について考えていきたいと思います。\n",
    "\n",
    "行列が関わってくると、逆伝播の計算が複雑そうに感じるかもしれません。\n",
    "\n",
    "しかし、そこまで難しく考える必要はありません。\n",
    "\n",
    "行列を対象とした逆伝播を求める場合は、行列の要素ごとに書き下すことによって、\n",
    "\n",
    "これまでと同様の手順で考えることができます。\n",
    "\n",
    "実際に書き下してみると以下のように考えることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\frac{∂L}{∂X} = \\frac{∂L}{∂Y}W^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{∂L}{∂W} = X^T\\frac{∂L}{∂Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式の$W^T$のTは転置を表します。\n",
    "\n",
    "転置とは、行列Wの要素である「$w_{ij}$」を「$w_{ji}$」の位置に入れ替えることを表し、以下のように書くことができます。\n",
    "\n",
    "$$\n",
    "    W = \\begin{pmatrix}\n",
    "    w_{11} & w_{12} & w_{13}\\\\\n",
    "    w_{21} & w_{22} & w_{23}\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    W^T = \\begin{pmatrix}\n",
    "    w_{11} & w_{21}\\\\\n",
    "    w_{12} & w_{22}\\\\\n",
    "    w_{13} & w_{23}\n",
    "    \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、Wの形状が２×３だとすると、転置行列$W^T$の形状は３×２になります。\n",
    "\n",
    "簡単に行ってしまうと、転置とは行数と列数を入れ替える作業になります。\n",
    "\n",
    "それでは、計算グラフの逆伝播を書いてみたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_11.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上図の計算グラフでは、各変数の形状に注意しましょう。\n",
    "\n",
    "特に、Xと$\\frac{∂L}{∂X}$は同じ形状であり、Wと$\\frac{∂L}{∂W}$は同じ形状になります。\n",
    "\n",
    "Xがn個の要素を持っているとすると、\n",
    "\n",
    "$$\n",
    "    X = (x_0, x_1, ・・・, x_n)\n",
    "$$\n",
    "\n",
    "と表すことができ、$\\frac{∂L}{∂X}$は以下のように表すことができます。\n",
    "\n",
    "$$\n",
    "    \\frac{∂L}{∂X} = \\bigl(\\frac{∂L}{∂x_0},\\frac{∂L}{∂x_1},・・・,\\frac{∂L}{∂x_n}\\bigr)\n",
    "$$\n",
    "\n",
    "これらの関係からXと$\\frac{∂L}{∂X}$の形状が同じになることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なぜ行列の形状に注意するのかというと、\n",
    "\n",
    "行列Aと行列Bの行列積を考える場合、Aの列数とBの行数を一致させる必要があるからです。\n",
    "\n",
    "例えば、$\\frac{∂L}{∂Y}$の形状が(3,)（１×３）、Wの形状が(2,3)（２×３）であるとき、\n",
    "\n",
    "$\\frac{∂L}{∂X}$の形状が(2,)（１×２）となるように、$\\frac{∂L}{∂Y}$とWの積を考えます。\n",
    "\n",
    "また、A・Bといった行列積の結果の形状は、Aの行数×Bの列数になることがわかっています。\n",
    "\n",
    "つまり、行列積の結果の形状が１×２になるということは、\n",
    "\n",
    "Aの行数が１であり、Bの列数が２であれば良いということになります。\n",
    "\n",
    "$\\frac{∂L}{∂Y}$の行数が１であるため、Aの部分にこの行列が当てはまります。\n",
    "\n",
    "では、BにWが当てはまるのでは？？\n",
    "\n",
    "と、なると思いますが、Wの列数は３であり、２ではありません。\n",
    "\n",
    "それだけでなく、$\\frac{∂L}{∂Y}$の列数は３であり、Wの行数が２であるため、このままでは行列積を計算できません。\n",
    "\n",
    "そこで、先ほど登場した転置行列を考えます。\n",
    "\n",
    "転置行列W<sup>T</sup>は形状が３×２となります。\n",
    "\n",
    "これは、行列積の条件を満たしていることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_7.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、\n",
    "\n",
    "#### 入力と重みの行列積やバイアスの加算を行うレイヤを**「Affineレイヤ」**と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.バッチ版Affineレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで説明してきたAffineレイヤは、入力であるXは一つのデータを対象としたものでした。\n",
    "\n",
    "この章では、N個のデータをまとめて順伝播する場合、つまり、バッチ版のAffineレイヤを考えます。\n",
    "\n",
    "バッチ版Affineレイヤの計算グラフは以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_8.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図4-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ版Affineレイヤは、入力Xの形状が(2,)（１×２）から(N,2)（N×２）となっただけになります。\n",
    "\n",
    "もちろん、入力の形状が(N,2)となったので、出力の形状も(3,)から(N,3)に変化します。\n",
    "\n",
    "また、バイアスを加える場合、以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5. ,  6.5,  6.5],\n",
       "       [ 8. , 10.5, 10.5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 2], [2, 3]])\n",
    "W = np.array([[1, 1.5, 1.5],[2, 2.5, 2.5]])\n",
    "\n",
    "Y = np.dot(X, W)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6. ,  8.5,  9.5],\n",
       "       [ 9. , 12.5, 13.5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、B = [1, 2, 3]を「X・W」の結果の各行に足し合わせていることがわかります。\n",
    "\n",
    "つまり、バイアスの加算は、各データ（２個のデータを入力する場合、１個目と２個目）に対して加算が行われます。\n",
    "\n",
    "そのため、逆伝播の際には、それぞれのデータの逆伝播の値がバイアスの要素に集約される必要があります。\n",
    "\n",
    "例えば、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バイアスの逆伝播は、その２個のデータに対しての微分を、データごとに合算して求めます。\n",
    "\n",
    "np.sumにおいて、「axis=0」と指定することによって、列方向の和を計算します。\n",
    "\n",
    "いきなり集約させると言われても…と困惑すると思うので、式を使って具体的に考えてみたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X・WとバイアスBの足し算を正確に書くと、\n",
    "$$\n",
    "    Y = X・W + 1B\n",
    "$$\n",
    "\n",
    "となります。\n",
    "\n",
    "ここでいう「１」は、全ての要素が１であるN×1の行列になります。（単位行列）\n",
    "\n",
    "\n",
    "例えば、５×１の場合、\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "となります。\n",
    "\n",
    "また、\n",
    "\n",
    "$$\n",
    "    B = \\begin{pmatrix}\n",
    "    2 & 3\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "とすると、\n",
    "\n",
    "$$\n",
    "    1B =\\\\\n",
    "    \\begin{pmatrix}\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "    2 & 3\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "    = \\begin{pmatrix}\n",
    "    2 & 3\\\\\n",
    "    2 & 3\\\\\n",
    "    2 & 3\\\\\n",
    "    2 & 3\\\\\n",
    "    2 & 3\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "となります。\n",
    "\n",
    "このように単位行列を使うことで、各列の要素が同じである行列を作成することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、前回のノートブックで、加算ノードの逆伝播は上流の値をそのまま下流に流すと説明しました。\n",
    "\n",
    "しかし、正確に説明すると、上流の値に1を乗算して、下流に流すというものになります。\n",
    "\n",
    "それでは、バッチ版Affineレイヤの加算ノードについてみていきたいと思います。\n",
    "\n",
    "上流の値は、$\\frac{∂L}{∂Y}$であり、この値に1を乗算します。\n",
    "\n",
    "また、$\\frac{∂L}{∂Y}$の形状は、N×３であり、$\\frac{∂L}{∂B}$の形状が１×３であることから、\n",
    "\n",
    "形状が１×Nである行列を乗算すれば良いことがわかります。\n",
    "\n",
    "また、加算ノードの逆伝播では１を乗算するので、１×Nの単位行列を乗算します。\n",
    "\n",
    "式で表すと、\n",
    "\n",
    "$$\n",
    "    \\frac{∂L}{∂B} = 1^{T}・\\frac{∂L}{∂Y}\n",
    "$$\n",
    "\n",
    "となります。\n",
    "\n",
    "これだけでは、集約する理由がわからないと思うので、具体的に計算してみたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は、N=2の場合を考えます。\n",
    "\n",
    "$$\n",
    "    1^T = \\begin{pmatrix}\n",
    "    1 & 1\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{∂L}{∂Y} = \\begin{pmatrix}\n",
    "    2 & 3 & 4\\\\\n",
    "    5 & 1 & 2\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    1^{T}・\\frac{∂L}{∂Y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    =\\begin{pmatrix}\n",
    "    1 & 1\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "    2 & 3 & 4\\\\\n",
    "    5 & 1 & 2\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\begin{pmatrix}\n",
    "    1\\times2+1\\times5 & 1\\times3+1\\times1 & 1\\times4+1\\times2\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\begin{pmatrix}\n",
    "    7 & 4 & 6\n",
    "    \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、バイアスの逆伝播を計算することができます。\n",
    "\n",
    "これは、先ほど説明した「axis=0」方向に加算することと同じ計算していることがわかると思います。\n",
    "\n",
    "これが、集約する正体になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上から、Affineの実装は以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列積の概念が導入され、少し複雑そうな感じがしますが、\n",
    "\n",
    "一つ一つ丁寧に考えると、実際のコードは簡単に書くことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題4-1】：バッチ版Affineレイヤを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######以下にコードを書いてください######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######「Shift+Enter」を押してください######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Softmax-with-Lossレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、出力層であるソフトマックス関数について説明したいと思います。\n",
    "\n",
    "ソフトマックス関数は以下の式で表すことができます。\n",
    "\n",
    "$$\n",
    "    y_k = \\frac{exp(a_k)}{\\sum_{i=1}^{n} exp(a_i)}\n",
    "$$\n",
    "\n",
    "ソフトマックス関数の出力は、０から１の間に収まり、出力の合計（y<sub>1</sub>+y<sub>2</sub>+・・・+y<sub>n</sub>）は１になりました。\n",
    "\n",
    "例えば、手書き数字認識の場合、Softmaxレイヤの出力は以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_12.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手書き数字認識は、10クラス分類を行うため、Softmaxレイヤへの入力は10個あります。\n",
    "\n",
    "（ニューラルネットワークで行う処理には、**推論**と**学習**の二つのフェーズがあります。ニューラルネットワークの推論では、通常Softmaxレイヤは使用しません。例えば、上図のようなネットワークで推論を行う場合、最後のAffineレイヤの出力（スコア）を認識結果として用います。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これから、Softmaxレイヤを実装していきますが、ここでは、損失かんすである交差エントロピー誤差も含めて、「Softmax-with-Lossレイヤ」という名前のレイヤで実装します。\n",
    "\n",
    "まずは、「Softmax-with-Lossレイヤ」の計算グラフを紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_10.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に書いてみましたが、「Softmax-with-Lossレイヤ」は複雑な構造を取っており、見えにくいかもしれません…\n",
    "\n",
    "気になる方は、「Softmax-with-Lossレイヤの計算グラフ」と調べてみてください。\n",
    "\n",
    "また、導出を説明すると大変なので、ここでは省略したいと思います。\n",
    "\n",
    "実際、計算グラフで行っている計算は難しくないので、導出も気になる方は、一つ一つ順を追って計算してみるといいかもしれません。\n",
    "\n",
    "この計算グラフですが、簡略化して書くと以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img4_2_9.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上図の計算グラフでは、ソフトマックス関数はSoftmaxレイヤとして、交差エントロピー誤差はCross Entropy Errorレイヤとして表記します。\n",
    "\n",
    "今回は、３クラス分類を行うと仮定し、前のレイヤから３つの入力を受け取るとします。\n",
    "\n",
    "Softmaxレイヤは、入力である$(a_1, a_2, a_3)$を正規化して、$(y_1, y_2, y_3)$を出力します。\n",
    "\n",
    "Cross Entropy Errorレイヤは、Softmaxの出力$(y_1, y_2, y_3)$と、教師ラベルの$(t_1, t_2, t_3)$を受け取り、それらのデータから損失Lを出力します。\n",
    "\n",
    "上図で注目すべきは、逆伝播の結果です。\n",
    "\n",
    "Softmaxレイヤからの逆伝播は、$(y_1-t_1, y_2-t_2, y_3-t_3)$というきれいな結果になっています。\n",
    "\n",
    "$(y_1, y_2, y_3)$はSoftmaxレイヤの出力、$(t_1, t_2, t_3)$は教師データなので、$(y_1-t_1, y_2-t_2, y_3-t_3)$は、Softmaxレイヤの出力と教師ラベルの差分になります。\n",
    "\n",
    "ニューラルネットワークの逆伝播では、この差分である誤差が前レイヤへ伝わっていくのです。\n",
    "\n",
    "これはニューラルネットワークの学習における重要な性質となっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、ニューラルネットワークの学習の目的をおさらいします。\n",
    "\n",
    "ニューラルネットワークの学習は、出力結果を教師ラベルに近づけるように、重みパラメータを調整することでした。\n",
    "\n",
    "そのため、ニューラルネットワークの出力と教師ラベルとの誤差を効率よく、前レイヤに伝える必要があります。\n",
    "\n",
    "先ほどの$(y_1-t_1, y_2-t_2, y_3-t_3)$という結果は、まさにSoftmaxレイヤの出力と教師ラベルの差であり、現在のニューラルネットワークの出力と教師ラベルの誤差を素直に表しているのです。\n",
    "\n",
    "（逆伝播が$(y_1-t_1, y_2-t_2, y_3-t_3)$というきれいな結果になりましたが、これは偶然ではなく、そうなるように交差エントロピー誤差という関数が設計されたのです。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで具体例を一つ考えてみましょう。\n",
    "\n",
    "例えば、教師ラベルが(0, 1, 0)であるデータに対して、Softmaxレイヤの出力が(0.3, 0.2, 0.5)であった場合を考えます。\n",
    "\n",
    "正解ラベルに対する確率は0.2つまり20%なので、この時点では正しく認識できていません。\n",
    "\n",
    "この場合、逆伝播は、\n",
    "\n",
    "$$\n",
    "     (0.3, 0.2, 0.5)-(0, 1, 0)\\\\\n",
    "    =(0.3, -0.8, 0.5)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "という大きな誤差を伝播することになります。\n",
    "\n",
    "この大きな誤差が前レイヤに伝播していくので、Softmaxレイヤよりも前のレイヤは、その大きな誤差から大きな内容を学習することになります。\n",
    "\n",
    "また、別の例として、教師ラベルが(0, 1, 0)であるデータに対して、Softmaxレイヤの出力が(0.01, 0.99, 0)の場合を考えます。\n",
    "\n",
    "正解ラベルに対する確率は0.99つまり99%なので、このネットワークはかなり正確な認識をしています。\n",
    "\n",
    "この場合、Softmaxレイヤからの逆伝播は、\n",
    "\n",
    "$$\n",
    "    (0.01, 0.99, 0) - (0, 1, 0)\\\\\n",
    "    =(0.01, -0.01, 0)\n",
    "$$\n",
    "\n",
    "という小さな誤差になります。\n",
    "\n",
    "この小さな誤差が前レイヤに伝播していきますが、その誤差は小さいため、Softmaxレイヤより前にあるレイヤが学習する内容も小さくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、Softmax-with-Lossレイヤの実装を行っていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_loss(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフでは複雑そうに見えましたが、実際の実装は簡単です。\n",
    "\n",
    "ここで「softmax」と「cross_entropy_error」といった二つの関数を用いましたが、\n",
    "\n",
    "これは今までのノートブックで実装した関数を用いています。\n",
    "\n",
    "また、逆伝播の際には、伝播する値をバッチの個数（batch_size）で割ることによって、データ１個あたりの誤差が前レイヤへ伝播します。\n",
    "\n",
    "バッチは束を表しているので、どれだけまとめて入力するかを表しています。\n",
    "\n",
    "例えば、画像を１００枚まとめて入力する場合、バッチの個数は１００になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題5-1】：Softmax-with-Lossレイヤを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######以下にコードを書いてください######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######「Shift+Enter」を押してください######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.誤差逆伝播法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今まで実装してきたレイヤを組み合わせることで、ニューラルネットワークを作成することができます。\n",
    "\n",
    "では、実装していきます。と言いたいところですが、もう一度ニューラルネットワークの学習について確認していきましょう。\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、ニューラルネットワークは、適応可能な重みとバイアスがあり、この重みとバイアスを訓練データに適応するように調整することを「学習」と呼びます。\n",
    "\n",
    "それでは、ニューラルネットワークの学習の手順についてみていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step1　ミニバッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データの中からランダムに一部のデータを選び出します。\n",
    "\n",
    "例えば、10000枚の訓練データから100枚をランダムに取り出すなど…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step2 勾配の算出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各重みパラメータに関する損失関数の勾配を求めます。\n",
    "\n",
    "前回は、勾配を求めるために、数値部分を使いましたが、今回は、このノートブックの主題である「誤差逆伝播法」を使って勾配を求めます。\n",
    "\n",
    "また、「誤差逆伝播法」を使うことで、高速に効率よく勾配を求めることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step3 パラメータの更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みパラメータを勾配方向に微少量だけ更新します。\n",
    "\n",
    "例えば、ある重みパラメータの勾配が「0.2」とすると、このパラメータはマイナス方向に更新すれば良いということになります。\n",
    "\n",
    "数学的に表すと、\n",
    "\n",
    "$$\n",
    "    w_1 = w_1 - 0.2h\n",
    "$$\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step4 繰り返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1からstep3までの手順を繰り返します。\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、実装していきたいと思います。\n",
    "\n",
    "今回は、２層のニューラルネットワーク（入力層＋隠れ層＋出力層）をTwoLayerNetとして実装していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 :\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "では、コードについて説明していきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この部分は、以前紹介したものになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「OrderedDict()」は、OrderedDictオブジェクトを生成する関数になります。\n",
    "\n",
    "OrderedDictは、順番付きのディクショナリになります。\n",
    "\n",
    "「順番付き」というのは、ディクショナリに追加した要素の順番を覚えることができる、ということを表しています。\n",
    "\n",
    "具体的に「OrderedDict()」を使ってみたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('k1', 1), ('k2', 2), ('k3', 3)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "od = OrderedDict()\n",
    "od['k1'] = 1\n",
    "od['k2'] = 2\n",
    "od['k3'] = 3\n",
    "\n",
    "od"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と、追加した順番にリストが作成されます。\n",
    "\n",
    "つまり、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.layers = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とすることによって、リストを作成し、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "により、Affineレイヤを追加することができます。\n",
    "\n",
    "また、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.layers['Relu1'] = Relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とすることによって、Affineレイヤの次に活性化関数レイヤであるReluレイヤを追加することができます。\n",
    "\n",
    "この時点で、Affineレイヤ→Reluレイヤという順番になっています。\n",
    "\n",
    "同様に、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とすることで、\n",
    "\n",
    "Affineレイヤ→Reluレイヤ→Affineレイヤ\n",
    "\n",
    "となります。\n",
    "\n",
    "このように、OrderedDict()を使うことで、レイヤを順番に格納することができます。\n",
    "\n",
    "逆伝播の際は、このリストを逆順にすれば良いので、とても便利なことがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax-with-Lossレイヤは最終レイヤなので、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.lastLayer = SoftmaxWithLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と、別名で生成します。\n",
    "\n",
    "このように、ニューラルネットワークの構成要素を「レイヤ」として実装したことで、ニューラルネットワークを簡単に構築することができました。\n",
    "\n",
    "この「レイヤ」としてモジュール化するのには大きなメリットがあります。\n",
    "\n",
    "それは、別のネットワークを作る場合、単に必要なレイヤを追加するだけでニューラルネットワークを作ることができるという点になります。\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、gradientについて少し説明したいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、上流からは1という値が下流に流れます。\n",
    "\n",
    "この値を「Softmax-with-Lossレイヤ」の逆伝播に流します。\n",
    "\n",
    "それが"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = self.lastLayer.backward(dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.layers.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "によって、リスト「self.layers」から値を取り出すことができます。\n",
    "\n",
    "例えば、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('k1', 1), ('k2', 2), ('k3', 3)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "od = OrderedDict()\n",
    "od['k1'] = 1\n",
    "od['k2'] = 2\n",
    "od['k3'] = 3\n",
    "\n",
    "od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values([1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、「od」のままでは、\n",
    "\n",
    "「'k1'」と1、「'k2'」と2、「'k3'」と3がそれぞれセットになっていますが、\n",
    "\n",
    "「values()」を使うことによって、1、2、3のみを取り出せています。\n",
    "\n",
    "つまり、「self.layers.values()」は、\n",
    "\n",
    "「Affine(self.params['W1'], self.params['b1'])」\n",
    "\n",
    "「Relu()」\n",
    "\n",
    "「Affine(self.params['W2'], self.params['b2'])」\n",
    "\n",
    "を取り出します。\n",
    "\n",
    "また、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(self.layers.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とすることで、\n",
    "\n",
    "[Affine(self.params['W1'], self.params['b1']), Relu(), Affine(self.params['W2'], self.params['b2'])]\n",
    "\n",
    "というようにリストを作成することができます。\n",
    "\n",
    "例えば、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od2 = list(od.values())\n",
    "od2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにすることで、ディクショナリから数値のみのリストを作成することができます。\n",
    "\n",
    "そして、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とすることで、リストの順番を逆にします。\n",
    "\n",
    "例えば、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "od2.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように元のリストが削除され、逆順にした数値が格納されています。\n",
    "\n",
    "つまり、layers.reverse()とすることで、\n",
    "\n",
    "Affine1→Relu→Affine2といった順番を\n",
    "\n",
    "Affine2→Relu→Affine1に変更し、逆伝播に対応させることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    dout = layer.backward(dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにfor文を回すことによって、逆伝播の値を計算していきます。\n",
    "\n",
    "また、doutを入力し、その結果をdoutに代入することで、上流から下流へと値が伝播していきます。\n",
    "\n",
    "また、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、Affineレイヤの「self.dW」や「self.db」を指定することによって、保持された逆伝播の値を取り出すことができます。\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.誤差逆伝播法の勾配確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで勾配を求める方法として、数値微分と誤差逆伝播法を説明してきました。\n",
    "\n",
    "数値微分の実装は比較的簡単ですが、計算が完了するまでに時間がかかってしまいます。\n",
    "\n",
    "それに対して、誤差逆伝播法は大量のパラメータが存在しているのにも関わらず、効率的に計算できました。\n",
    "\n",
    "そのため、勾配を求める際には誤差逆伝播法を使うことをおすすめします。\n",
    "\n",
    "では、なぜ数値微分である「numerical_gradient」を上記のコードに記載したのでしょうか。\n",
    "\n",
    "それは、誤差逆伝播法の実装の正しさを確認することができるからです。\n",
    "\n",
    "先ほども説明しましたが、数値微分の実装は比較的簡単であるため、ミスが起きにくいものとなっています。\n",
    "\n",
    "それに対し、誤差逆伝播法の実装は複雑になるためミスが起きやすくなってしまいます。\n",
    "\n",
    "そこで、数値微分と誤差逆伝播法の2通りで勾配を計算し比較することで、誤差逆伝播法の実装の正確さを確認することができます。\n",
    "\n",
    "このように2通りの方法で求めた勾配を比較することを**勾配確認**と言います。\n",
    "\n",
    "それでは、勾配確認について実装していきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "mnist = datasets.fetch_openml('mnist_784', version=1)\n",
    "data = mnist.data\n",
    "std_data = data/255\n",
    "label = mnist.target\n",
    "label = [int(x) for x in label]\n",
    "one_hot_label = np.identity(10)[label]\n",
    "x_train, x_test, t_train, t_test = train_test_split(std_data, one_hot_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, t):\n",
    "    if y.ndim == 1 :\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a, axis=-1, keepdims=True)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a, axis=-1, keepdims=True)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:5.213789520934346e-10\n",
      "b1:3.428092050821509e-09\n",
      "W2:7.189420569550162e-09\n",
      "b2:1.591348380305946e-07\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key] ))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、今までと同様に「MNISTデータ」を読み込みます。\n",
    "\n",
    "そして、訓練データの一部を使って、数値微分と誤差逆伝播法の2通りで勾配を計算します。\n",
    "\n",
    "確認するだけなので、バッチサイズを小さくし、「numerical_gradient」でもそこまで時間がかからないようにします。\n",
    "\n",
    "ここでは、勾配の誤差として、各重みパラメータにおける要素の差の絶対値（np.abs()）を求め、その平均（np.average）を計算します。\n",
    "\n",
    "これらの結果が\n",
    "\n",
    "W1:5.213789520934346e-10  \n",
    "b1:3.428092050821509e-09  \n",
    "W2:7.189420569550162e-09  \n",
    "b2:1.591348380305946e-07  \n",
    "\n",
    "になります。\n",
    "\n",
    "これは、かなり小さな値となっていることがわかります。\n",
    "\n",
    "つまり、誤差逆伝播法で求めた勾配も正しい結果であることがわかり、誤差逆伝播法の実装に誤りがないことの信頼性が増します。\n",
    "\n",
    "数値微分と誤差逆伝播法で求めた勾配の誤差が０になることはあまりありません。\n",
    "\n",
    "というのも、コンピュータの計算は有限の精度で行われることに関係します。\n",
    "\n",
    "そのため、誤差がかなり小さな値になっていれば問題ないということが言えます。\n",
    "\n",
    "逆に言えば、誤差が大きな値を取っていると、誤差逆伝播法の実装に誤りがあるということになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは最後に、誤差逆伝播法を使ったニューラルネットワークの学習の実装をしていきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epoch: 0.10232142857142858 0.09892857142857144\n",
      "1epoch: 0.9051964285714286 0.8992142857142857\n",
      "2epoch: 0.9230892857142857 0.9169285714285714\n",
      "3epoch: 0.9364464285714286 0.9302142857142857\n",
      "4epoch: 0.9458571428571428 0.9375\n",
      "5epoch: 0.9482857142857143 0.9416428571428571\n",
      "6epoch: 0.9556428571428571 0.9462142857142857\n",
      "7epoch: 0.9615892857142857 0.952\n",
      "8epoch: 0.9639464285714285 0.9531428571428572\n",
      "9epoch: 0.9653035714285715 0.9565\n",
      "10epoch: 0.9688392857142857 0.9585\n",
      "11epoch: 0.9721071428571428 0.9617142857142857\n",
      "12epoch: 0.9739821428571429 0.9632857142857143\n",
      "13epoch: 0.9765 0.9652142857142857\n",
      "14epoch: 0.9769107142857143 0.9647142857142857\n",
      "15epoch: 0.9774642857142857 0.964\n",
      "16epoch: 0.9795535714285715 0.9657857142857142\n",
      "17epoch: 0.9805892857142857 0.9669285714285715\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        n = i / iter_per_epoch\n",
    "        n = int(n)\n",
    "        print(str(n) + \"epoch:\", train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、実行するとすぐに精度の表示が開始されます。\n",
    "\n",
    "誤差逆伝播法を使った学習は早いことがわかります。\n",
    "\n",
    "また、訓練画像やテスト画像の両方に対して、高い精度で分類できていることが確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックでは、計算グラフや逆伝播をニューラルネットワークに応用した内容を学びました。\n",
    "\n",
    "また、ニューラルネットワークの各層で行う処理をレイヤという形で実装することにより、組み合わせるだけでニューラルネットワークを実装することができました。\n",
    "\n",
    "\n",
    "その他に、各レイヤにはforwardとbackwardというメソッドが実装されており、データを順方向と逆方向に伝播することで、重みのパラメータの勾配を効率的に求めることができました。\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.問題集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題10-1】以下の条件に従って、3層のニューラルネットワークを作成してください。\n",
    "\n",
    "・一層目の活性化関数はsigmoid関数\n",
    "\n",
    "・二層目の活性化関数はrelu関数\n",
    "\n",
    "・出力層の活性化関数はsoftmax関数\n",
    "\n",
    "・損失関数はcross_entropy_loss関数\n",
    "\n",
    "・weight_init_std=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######以下にコードを書いてください######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######「Shift + Enter」を押してください######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題10-2】問題10-1で作成したニューラルネットワークモデルで、手書き数字認識を実装してください。（学習した後、テストデータでモデルの評価を行うこと。）隠れ層１のニューロン数は50、隠れ層２のニューロン数は100としてください。\n",
    "\n",
    "また、cross_entropy_lossやnumerical_gradientはコピペしても構いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######以下にコードを書いてください######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######「Shift + Enter」を押してください######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.DeepLearningの扉の向こうには"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お疲れ様でした。\n",
    "\n",
    "これで「DeepLearningの扉」は終わりになります。\n",
    "\n",
    "全てのノートブックを通して、パーセプトロンといった原点の内容からニューラルネットワークや手書き数字認識、誤差逆伝播法など発展的な内容まで、DeepLearningにおける様々な基礎知識を学んできました。\n",
    "\n",
    "これらを学ぶことによって、自分の解きたい問題を設定し、それに対してネットワークモデルを組むことができます。（もちろん解く問題にも限りがありますが…）\n",
    "\n",
    "また、これらの基礎知識がないと、この先の内容を学ぼうとしても、これは何なんだろう？？と手が止まってしまうと思います。\n",
    "\n",
    "調べながら進めても知らないことばかり。\n",
    "\n",
    "そんな勉強嫌ですよね。\n",
    "\n",
    "そんな風にならないように手助けするのが\n",
    "\n",
    "この\n",
    "\n",
    "「DeepLearningの扉」\n",
    "\n",
    "なのです。\n",
    "\n",
    "ノートブックに登場する基礎知識を頭に入れてしまえば、扉の先に広がるDeepLearningの世界にもどんどん踏み込むことができると思います。\n",
    "\n",
    "まだ見ぬDeepLearningの世界を求めて、頑張りましょう！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
